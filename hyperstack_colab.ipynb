{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SAM 3 Hyperstack Test Notebook\n",
                "This notebook tests the `hyperstack.py` script for SAM 3 segmentation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install ultralytics huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Authenticate with Hugging Face (Required for SAM 3 model)\n",
                "from huggingface_hub import login\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Create prompts.json\n",
                "import json\n",
                "\n",
                "prompts_data = {\n",
                "  \"bottomwear_men_cargo_pants\": \"cargo pants\",\n",
                "  \"bottomwear_men_chinos\": \"chino pants\",\n",
                "  \"bottomwear_men_dhoti\": \"dhoti pants\",\n",
                "  \"bottomwear_men_formal_trousers\": \"formal trousers\",\n",
                "  \"bottomwear_men_jeans\": \"jeans\",\n",
                "  \"bottomwear_men_joggers\": \"jogger pants\",\n",
                "  \"bottomwear_men_relaxed_jeans\": \"relaxed fit jeans\",\n",
                "  \"bottomwear_men_shorts\": \"shorts\",\n",
                "  \"bottomwear_men_skinny_jeans\": \"skinny jeans\",\n",
                "  \"bottomwear_men_slim_jeans\": \"slim fit jeans\",\n",
                "  \"bottomwear_men_track_pants\": \"track pants\",\n",
                "  \"bottomwear_women_bootcut_jeans\": \"bootcut jeans\",\n",
                "  \"bottomwear_women_cargos\": \"cargo pants\",\n",
                "  \"bottomwear_women_jeans\": \"jeans\",\n",
                "  \"bottomwear_women_joggers\": \"jogger pants\",\n",
                "  \"bottomwear_women_leggings\": \"leggings\",\n",
                "  \"bottomwear_women_midi_skirt\": \"midi skirt\",\n",
                "  \"bottomwear_women_mini_skirt\": \"mini skirt\",\n",
                "  \"bottomwear_women_mom_fit_jeans\": \"mom jeans\",\n",
                "  \"bottomwear_women_shorts\": \"shorts\",\n",
                "  \"bottomwear_women_skinny_jeans\": \"skinny jeans\",\n",
                "  \"bottomwear_women_skirt\": \"skirt\",\n",
                "  \"bottomwear_women_trousers\": \"trousers\",\n",
                "  \"bottomwear_women_wide-leg_jeans\": \"wide leg jeans\",\n",
                "  \"ethnic_wear_men_long_kurta\": \"long kurta\",\n",
                "  \"ethnic_wear_men_nehru_jacket\": \"nehru jacket\",\n",
                "  \"ethnic_wear_men_sherwani\": \"sherwani\",\n",
                "  \"ethnic_wear_men_short_kurta\": \"short kurta\",\n",
                "  \"ethnic_wear_men_waistcoat\": \"waistcoat\",\n",
                "  \"ethnic_wear_women_anarkali\": \"anarkali suit\",\n",
                "  \"ethnic_wear_women_dupatta\": \"dupatta\",\n",
                "  \"ethnic_wear_women_kurti\": \"kurti\",\n",
                "  \"ethnic_wear_women_lehenga_choli\": \"lehenga choli\",\n",
                "  \"ethnic_wear_women_palazzo_set\": \"palazzo set\",\n",
                "  \"ethnic_wear_women_salwar_suit\": \"salwar suit\",\n",
                "  \"ethnic_wear_women_saree\": \"saree\",\n",
                "  \"ethnic_wear_women_sharara\": \"sharara set\",\n",
                "  \"fusion_wear_women_indo-western_dress\": \"indo western dress\",\n",
                "  \"fusion_wear_women_kaftan\": \"kaftan\",\n",
                "  \"general_unisex_hoodie\": \"hoodie\",\n",
                "  \"general_unisex_joggers\": \"jogger pants\",\n",
                "  \"general_unisex_oversized_t-shirt\": \"oversized t-shirt\",\n",
                "  \"general_unisex_raincoat\": \"raincoat\",\n",
                "  \"general_unisex_sweatshirt\": \"sweatshirt\",\n",
                "  \"general_unisex_tracksuit\": \"tracksuit\",\n",
                "  \"sleepwear_men_lounge_pants\": \"lounge pants\",\n",
                "  \"sleepwear_men_pyjama_set\": \"pyjama set\",\n",
                "  \"sleepwear_women_night_suit\": \"night suit\",\n",
                "  \"sleepwear_women_nightgown\": \"nightgown\",\n",
                "  \"sleepwear_women_robes\": \"robe\",\n",
                "  \"sportswear_men_gym_shorts\": \"gym shorts\",\n",
                "  \"sportswear_men_sports_t-shirt\": \"sports t-shirt\",\n",
                "  \"sportswear_men_tracksuit\": \"tracksuit\",\n",
                "  \"sportswear_women_gym_leggings\": \"gym leggings\",\n",
                "  \"sportswear_women_gym_top\": \"gym top\",\n",
                "  \"sportswear_women_sports_bra\": \"sports bra\",\n",
                "  \"topwear_men_blazer\": \"blazer\",\n",
                "  \"topwear_men_bomber_jacket\": \"bomber jacket\",\n",
                "  \"topwear_men_casual_shirt\": \"casual shirt\",\n",
                "  \"topwear_men_coat\": \"coat\",\n",
                "  \"topwear_men_denim_jacket\": \"denim jacket\",\n",
                "  \"topwear_men_denim_shirt\": \"denim shirt\",\n",
                "  \"topwear_men_formal_shirt\": \"formal shirt\",\n",
                "  \"topwear_men_graphic_t-shirt\": \"graphic t-shirt\",\n",
                "  \"topwear_men_henley_t-shirt\": \"henley t-shirt\",\n",
                "  \"topwear_men_hoodie\": \"hoodie\",\n",
                "  \"topwear_men_leather_jacket\": \"leather jacket\",\n",
                "  \"topwear_men_linen_shirt\": \"linen shirt\",\n",
                "  \"topwear_men_oversized_t-shirt\": \"oversized t-shirt\",\n",
                "  \"topwear_men_polo_t-shirt\": \"polo t-shirt\",\n",
                "  \"topwear_men_printed_shirt\": \"printed shirt\",\n",
                "  \"topwear_men_puffer_jacket\": \"puffer jacket\",\n",
                "  \"topwear_men_sweater\": \"sweater\",\n",
                "  \"topwear_men_sweatshirt\": \"sweatshirt\",\n",
                "  \"topwear_men_t-shirt\": \"t-shirt\",\n",
                "  \"topwear_men_windcheater\": \"windcheater jacket\",\n",
                "  \"topwear_women_blouse\": \"blouse\",\n",
                "  \"topwear_women_bodysuit\": \"bodysuit\",\n",
                "  \"topwear_women_bomber_jacket\": \"bomber jacket\",\n",
                "  \"topwear_women_camisole\": \"camisole\",\n",
                "  \"topwear_women_crop_top\": \"crop top\",\n",
                "  \"topwear_women_denim_jacket\": \"denim jacket\",\n",
                "  \"topwear_women_hoodie\": \"hoodie\",\n",
                "  \"topwear_women_leather_jacket\": \"leather jacket\",\n",
                "  \"topwear_women_shirt\": \"shirt\",\n",
                "  \"topwear_women_sweater\": \"sweater\",\n",
                "  \"topwear_women_sweatshirt\": \"sweatshirt\",\n",
                "  \"topwear_women_t-shirt\": \"t-shirt\",\n",
                "  \"topwear_women_top\": \"top\",\n",
                "  \"topwear_women_trench_coat\": \"trench coat\",\n",
                "  \"tunic_nan_women\": \"tunic\",\n",
                "  \"western_wear_women_bodycon_dress\": \"bodycon dress\",\n",
                "  \"western_wear_women_jumpsuit\": \"jumpsuit\",\n",
                "  \"western_wear_women_maxi_dress\": \"maxi dress\",\n",
                "  \"western_wear_women_midi_dress\": \"midi dress\",\n",
                "  \"western_wear_women_mini_dress\": \"mini dress\",\n",
                "  \"western_wear_women_playsuit\": \"playsuit\",\n",
                "  \"western_wear_women_shirt_dress\": \"shirt dress\",\n",
                "  \"western_wear_women_wrap_dress\": \"wrap dress\"\n",
                "}\n",
                "\n",
                "with open('prompts.json', 'w') as f:\n",
                "    json.dump(prompts_data, f, indent=2)\n",
                "print('prompts.json created')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Create hyperstack.py (Copying script content)\n",
                "script_content = r'''#!/usr/bin/env python3\n",
                "import os\n",
                "import json\n",
                "import argparse\n",
                "import subprocess\n",
                "import shutil\n",
                "import cv2\n",
                "import torch\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from ultralytics.models.sam import SAM3SemanticPredictor\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "# ------------- Defaults -------------\n",
                "DEFAULT_INPUT_ROOT = \"input_data\"\n",
                "DEFAULT_OUTPUT_ROOT = \"output_data\"\n",
                "DEFAULT_MODEL_PATH = \"sam3.pt\"\n",
                "DEFAULT_MAX_RES = 1024\n",
                "DEFAULT_LOG_EVERY = 10\n",
                "\n",
                "# S3 Defaults (Placeholder)\n",
                "DEFAULT_S3_MODEL_URI = \"s3://my-bucket/models/\" # Not really used for manual HF download but kept for arg compat\n",
                "DEFAULT_S3_INPUT_URI = \"s3://my-bucket/input/\"\n",
                "DEFAULT_S3_SAMPLE_URI = \"s3://my-bucket/sample_data/\"\n",
                "DEFAULT_S3_OUTPUT_URI = \"s3://my-bucket/output/\"\n",
                "\n",
                "def ensure_dir(path):\n",
                "    if not os.path.exists(path):\n",
                "        os.makedirs(path)\n",
                "\n",
                "# ------------- Utils -------------\n",
                "def load_prompts(prompts_path: str) -> dict:\n",
                "    if not os.path.exists(prompts_path):\n",
                "        # Fallback if file not found, though user validation should catch this\n",
                "        print(f\"⚠️ Warning: prompts.json not found at {prompts_path}\")\n",
                "        return {}\n",
                "    with open(prompts_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "def mask_to_yolo_polygon(mask, width, height):\n",
                "    \"\"\"\n",
                "    Convert a binary mask (uint8) to a YOLO polygon string.\n",
                "    Format: <class_id> <x1> <y1> <x2> <y2> ...\n",
                "    Normalized coordinates [0, 1].\n",
                "    \"\"\"\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if not contours:\n",
                "        return \"\"\n",
                "\n",
                "    # Find the largest contour by area\n",
                "    c = max(contours, key=cv2.contourArea)\n",
                "    \n",
                "    # Needs a minimum number of points\n",
                "    if len(c) < 3:\n",
                "        return \"\"\n",
                "\n",
                "    # Normalize points\n",
                "    polygon = []\n",
                "    for point in c:\n",
                "        x, y = point[0]\n",
                "        polygon.append(f\"{x / width:.6f}\")\n",
                "        polygon.append(f\"{y / height:.6f}\")\n",
                "    \n",
                "    return \" \".join(polygon)\n",
                "\n",
                "def download_sam3_model(model_path):\n",
                "    \"\"\"Manually downloads sam3.pt from Hugging Face if not present.\"\"\"\n",
                "    if os.path.exists(model_path):\n",
                "        print(f\"✅ Model found at {model_path}\")\n",
                "        return\n",
                "\n",
                "    print(f\"⬇️ Model {model_path} not found. Downloading from Hugging Face (facebook/sam3)...\")\n",
                "    try:\n",
                "        # We download to current dir, which is usually where model_path points (e.g. \"sam3.pt\")\n",
                "        # If model_path has a dir component, we should respect it, but typically it's just a filename.\n",
                "        local_dir = os.path.dirname(model_path) or \".\"\n",
                "        filename = os.path.basename(model_path)\n",
                "        \n",
                "        hf_hub_download(repo_id=\"facebook/sam3\", filename=filename, local_dir=local_dir)\n",
                "        print(\"✅ Download complete.\")\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Failed to download model: {e}\")\n",
                "        print(\"Ensure you have set HF_TOKEN env var or run 'huggingface-cli login'.\")\n",
                "        print(\"Also ensure you accepted the license at https://huggingface.co/facebook/sam3\")\n",
                "        raise e\n",
                "\n",
                "# ------------- Main Processor -------------\n",
                "\n",
                "from collections import defaultdict\n",
                "\n",
                "class Stats:\n",
                "    def __init__(self):\n",
                "        self.total = 0\n",
                "        self.processed = 0\n",
                "        self.skipped = 0\n",
                "\n",
                "def process_batch(args, predictor, prompts_dict, device):\n",
                "    stats = Stats()\n",
                "    category_metadata = defaultdict(list)\n",
                "    \n",
                "    # Gather all images\n",
                "    # We support flat directory or category-subdirectories. \n",
                "    # Logic: Walk input dir.\n",
                "    \n",
                "    image_files = []\n",
                "    # (path, filename, category)\n",
                "    \n",
                "    for root, dirs, files in os.walk(args.input):\n",
                "        for file in files:\n",
                "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
                "                full_path = os.path.join(root, file)\n",
                "                \n",
                "                # Infer category\n",
                "                # 1. Check parent folder name\n",
                "                parent_name = os.path.basename(root)\n",
                "                category = parent_name if parent_name in prompts_dict else None\n",
                "                \n",
                "                # 2. If not found, check filename matching keys\n",
                "                if not category:\n",
                "                    for key in prompts_dict:\n",
                "                        if file.startswith(key):\n",
                "                            category = key\n",
                "                            break\n",
                "                \n",
                "                if category:\n",
                "                    image_files.append((full_path, file, category))\n",
                "    \n",
                "    stats.total = len(image_files)\n",
                "    print(f\"Found {stats.total} images to process.\")\n",
                "\n",
                "    for i, (img_path, filename, category) in enumerate(image_files):\n",
                "        if i % args.log_every == 0:\n",
                "            print(f\"[{i}/{stats.total}] Processing {filename} ({category})...\")\n",
                "\n",
                "        prompt_text = prompts_dict.get(category)\n",
                "        if not prompt_text:\n",
                "            print(f\"Skipping {filename}: No prompt found for category '{category}'\")\n",
                "            stats.skipped += 1\n",
                "            continue\n",
                "\n",
                "        base_name = os.path.splitext(filename)[0]\n",
                "        \n",
                "        # Output structure: preserve subdirectory structure relative to input? \n",
                "        # Or simple flat output as per original script logic which seemed to use output root directly or subdirs.\n",
                "        # Let's mirror the input folder structure if it was inside a category folder, \n",
                "        # otherwise put it in category folder in output.\n",
                "        \n",
                "        # Simplified: Output to args.output/<category>/\n",
                "        cat_out_dir = os.path.join(args.output, category)\n",
                "        ensure_dir(cat_out_dir)\n",
                "        \n",
                "        out_mask_path = os.path.join(cat_out_dir, f\"{base_name}.png\")\n",
                "        out_label_path = os.path.join(cat_out_dir, f\"{base_name}.txt\")\n",
                "        out_overlay_path = os.path.join(cat_out_dir, f\"{base_name}_overlay.jpg\")\n",
                "        \n",
                "        if os.path.exists(out_mask_path) and not args.upload_output: # upload-output flag reused as \"force overwrite\" maybe?\n",
                "             # For now, just continue if exists unless forced? Script doesn't say. Let's process.\n",
                "             pass\n",
                "\n",
                "        try:\n",
                "            # --- SAM 3 Prediction ---\n",
                "            # 1. Set Image\n",
                "            # SAM3SemanticPredictor handles loading.\n",
                "            predictor.set_image(img_path)\n",
                "            \n",
                "            # 2. Predict with text\n",
                "            print(f\"DEBUG: Using prompt='{prompt_text}' for {filename}\")\n",
                "            results = predictor(text=[prompt_text])\n",
                "            \n",
                "            # results is a list of Results objects (one per prompt text? or one per image?)\n",
                "            # Since we set_image once and passed 1 text prompt, we likely get 1 result object or list of objects found for that prompt.\n",
                "            # Ultralytics results: result[0].masks.data is torch tensor\n",
                "            \n",
                "            final_mask = None\n",
                "            \n",
                "            if results and results[0].masks is not None:\n",
                "                # Combine all found instances for this prompt into one binary mask\n",
                "                # masks.data is (N, H, W) where N is number of objects found\n",
                "                masks_tensor = results[0].masks.data # GPU tensor usually\n",
                "                \n",
                "                if masks_tensor.numel() > 0:\n",
                "                    # Union of all masks\n",
                "                    files_mask = torch.any(masks_tensor, dim=0).squeeze().cpu().numpy().astype(np.uint8) * 255\n",
                "                    final_mask = files_mask\n",
                "            \n",
                "            if final_mask is None:\n",
                "               # No detection\n",
                "               stats.skipped += 1\n",
                "               continue\n",
                "\n",
                "            # --- Save Outputs ---\n",
                "            \n",
                "            # 1. Mask\n",
                "            cv2.imwrite(out_mask_path, final_mask)\n",
                "            \n",
                "            original_img = cv2.imread(img_path)\n",
                "            H, W = original_img.shape[:2]\n",
                "            \n",
                "            # 2. YOLO Label\n",
                "            poly_str = mask_to_yolo_polygon(final_mask, W, H)\n",
                "            if poly_str:\n",
                "                # Class ID 0 for all? Or specific id? \n",
                "                # Originally we didn't have class mapping. Let's assume 0.\n",
                "                with open(out_label_path, \"w\") as f:\n",
                "                    f.write(f\"0 {poly_str}\\n\")\n",
                "            \n",
                "            # 3. Visualization\n",
                "            # Draw polygon on original image\n",
                "            if poly_str:\n",
                "                coords = [float(x) for x in poly_str.split()]\n",
                "                # Rescale to pixels\n",
                "                pts = []\n",
                "                for j in range(0, len(coords), 2):\n",
                "                    px = int(coords[j] * W)\n",
                "                    py = int(coords[j+1] * H)\n",
                "                    pts.append([px, py])\n",
                "                \n",
                "                pts = np.array(pts, np.int32)\n",
                "                pts = pts.reshape((-1, 1, 2))\n",
                "                cv2.polylines(original_img, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
                "                cv2.imwrite(out_overlay_path, original_img)\n",
                "\n",
                "            # 4. Copy Original Image (Ensure YOLO dataset completeness)\n",
                "            out_image_path = os.path.join(cat_out_dir, f\"{base_name}{os.path.splitext(filename)[1]}\")\n",
                "            if not os.path.exists(out_image_path):\n",
                "                shutil.copy2(img_path, out_image_path)\n",
                "\n",
                "            # 5. Metadata Collection\n",
                "            # Extract confidence (max of detected objects)\n",
                "            conf_score = 0.0\n",
                "            try:\n",
                "                if results[0].boxes is not None and results[0].boxes.conf is not None:\n",
                "                    if results[0].boxes.conf.numel() > 0:\n",
                "                        conf_score = float(results[0].boxes.conf.max().item())\n",
                "            except Exception:\n",
                "                pass # Default to 0.0 if extraction fails\n",
                "\n",
                "            # Rule: < 0.60 -> human review\n",
                "            human_review = conf_score < 0.60\n",
                "            \n",
                "            category_metadata[category].append({\n",
                "                \"image\": filename,\n",
                "                \"confidence\": round(conf_score, 4),\n",
                "                \"human_review_needed\": human_review\n",
                "            })\n",
                "\n",
                "            stats.processed += 1\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {filename}: {e}\")\n",
                "            stats.skipped += 1\n",
                "            continue\n",
                "\n",
                "    # --- Write Metadata JSONs ---\n",
                "    for cat, meta_list in category_metadata.items():\n",
                "        if not meta_list:\n",
                "            continue\n",
                "        \n",
                "        meta_path = os.path.join(args.output, cat, \"metadata.json\")\n",
                "        try:\n",
                "            # If exists, maybe load and append? For now, we overwrite or it's a batch job.\n",
                "            # Let's assume batch job per run.\n",
                "            with open(meta_path, \"w\") as f:\n",
                "                json.dump(meta_list, f, indent=2)\n",
                "            print(f\"Saved metadata to {meta_path}\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to save metadata for {cat}: {e}\")\n",
                "\n",
                "    return stats\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    parser = argparse.ArgumentParser()\n",
                "    \n",
                "    # SageMaker / General Arguments\n",
                "    parser.add_argument(\"--mode\", type=str, default=\"process\", choices=[\"process\", \"sample\"], help=\"Execution mode\")\n",
                "    parser.add_argument(\"--download-model\", action=\"store_true\") # Kept for compat, triggers HF download\n",
                "    parser.add_argument(\"--download-input\", action=\"store_true\") # Kept for compat (S3 download)\n",
                "    parser.add_argument(\"--upload-output\", action=\"store_true\")\n",
                "    \n",
                "    parser.add_argument(\"--input\", default=DEFAULT_INPUT_ROOT)\n",
                "    parser.add_argument(\"--output\", default=DEFAULT_OUTPUT_ROOT)\n",
                "    parser.add_argument(\"--model\", default=DEFAULT_MODEL_PATH)\n",
                "    parser.add_argument(\"--prompts\", default=\"prompts.json\")\n",
                "    \n",
                "    parser.add_argument(\"--max-res\", type=int, default=DEFAULT_MAX_RES)\n",
                "    parser.add_argument(\"--log-every\", type=int, default=DEFAULT_LOG_EVERY)\n",
                "    \n",
                "    parser.add_argument(\"--s3-model-uri\", default=DEFAULT_S3_MODEL_URI)\n",
                "    parser.add_argument(\"--s3-input-uri\", default=DEFAULT_S3_INPUT_URI)\n",
                "    parser.add_argument(\"--s3-sample-uri\", default=DEFAULT_S3_SAMPLE_URI)\n",
                "    parser.add_argument(\"--s3-output-uri\", default=DEFAULT_S3_OUTPUT_URI)\n",
                "    \n",
                "    # SAM 3 Configs\n",
                "    parser.add_argument(\"--box-threshold\", type=float, default=0.25, help=\"Confidence threshold (maps to conf in SAM3)\")\n",
                "\n",
                "    # args = parser.parse_args()\n",
                "    # MOCK ARGS FOR COLAB\n",
                "    args = parser.parse_args([\"--input\", \"input_data\", \"--output\", \"output_data\", \"--download-model\", \"--box-threshold\", \"0.25\"])\n",
                "\n",
                "    ensure_dir(args.input)\n",
                "    ensure_dir(args.output)\n",
                "    \n",
                "    # 1. Download Model (if requested or missing)\n",
                "    download_sam3_model(args.model)\n",
                "\n",
                "    # 2. Download Input (Placeholder S3 logic - user should handle via s3 sync or similar for now if strictly local, \n",
                "    # but keeping original flow if they use the flags)\n",
                "    # 2. Download Input (Placeholder S3 logic - user should handle via s3 sync or similar for now if strictly local, \n",
                "    # but keeping original flow if they use the flags)\n",
                "    if args.download_input:\n",
                "        if args.mode == \"sample\":\n",
                "            print(f\"Downloading samples from {args.s3_sample_uri}...\")\n",
                "            subprocess.call([\"aws\", \"s3\", \"sync\", args.s3_sample_uri, args.input, \"--quiet\"])\n",
                "        else:\n",
                "            print(f\"Downloading inputs from {args.s3_input_uri}...\")\n",
                "            subprocess.call([\"aws\", \"s3\", \"sync\", args.s3_input_uri, args.input, \"--quiet\"])\n",
                "    \n",
                "    # 3. Load Prompts\n",
                "    prompts = load_prompts(args.prompts)\n",
                "    print(f\"Loaded {len(prompts)} category prompts.\")\n",
                "\n",
                "    # 4. Initialize SAM 3 Predictor\n",
                "    print(f\"Initializing SAM 3 Predictor with model={args.model}...\")\n",
                "    # Matches user's working Colab snippet\n",
                "    overrides = dict(\n",
                "        conf=args.box_threshold, \n",
                "        task=\"segment\",\n",
                "        mode=\"predict\",\n",
                "        model=args.model,\n",
                "        half=True,  # FP16\n",
                "        save=True,  # Ultralytics quirk? Maybe needed for internal pipeline\n",
                "    )\n",
                "    \n",
                "    # Check device\n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    print(f\"Using device: {device}\")\n",
                "    \n",
                "    try:\n",
                "        predictor = SAM3SemanticPredictor(overrides=overrides)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to initialize predictor: {e}\")\n",
                "        exit(1)\n",
                "\n",
                "    # 5. Process\n",
                "    stats = process_batch(args, predictor, prompts, device)\n",
                "    \n",
                "    print(f\"\\n--- Batch Complete ---\")\n",
                "    print(f\"Total: {stats.total}\")\n",
                "    print(f\"Processed: {stats.processed}\")\n",
                "    print(f\"Skipped/Failed: {stats.skipped}\")\n",
                "\n",
                "    # 6. Upload Output\n",
                "    if args.upload_output and args.s3_output_uri != DEFAULT_S3_OUTPUT_URI:\n",
                "        print(f\"Uploading output to {args.s3_output_uri}...\")\n",
                "        subprocess.call([\"aws\", \"s3\", \"sync\", args.output, args.s3_output_uri, \"--quiet\"])\n",
                "    \n",
                "    print(\"Done!\")\n",
                "'''\n",
                "\n",
                "with open(\"hyperstack.py\", \"w\") as f:\n",
                "    f.write(script_content)\n",
                "print(\"hyperstack.py created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Create Sample Image (Download from your sample data)\n",
                "import os\n",
                "import wget\n",
                "\n",
                "# Mock folder for \"bottomwear_men_cargo_pants\"\n",
                "os.makedirs(\"input_data/bottomwear_men_cargo_pants\", exist_ok=True)\n",
                "\n",
                "# Download a sample cargo pants image\n",
                "sample_url = \"https://media.istockphoto.com/id/499268323/photo/cargo-pants.jpg?s=612x612&w=0&k=20&c=h7tD_1uS0eJk222tqYc0Mjgx2n9wG3p7p5L9rJ8b2zI=\" # Example placeholder URL\n",
                "save_path = \"input_data/bottomwear_men_cargo_pants/sample_cargo.jpg\"\n",
                "\n",
                "if not os.path.exists(save_path):\n",
                "    # Attempt download or create a dummy if URL fails (replace with your own valid image URL for real test)\n",
                "    try:\n",
                "        import requests\n",
                "        response = requests.get(\"https://images.unsplash.com/photo-1541099649105-f69ad21f3246?ixlib=rb-1.2.1&auto=format&fit=crop&w=800&q=80\", timeout=5)\n",
                "        with open(save_path, 'wb') as f:\n",
                "             f.write(response.content)\n",
                "        print(\"Sample image downloaded.\")\n",
                "    except:\n",
                "        print(\"⚠️ Could not download sample image. Please upload a 'test.jpg' to 'input_data/bottomwear_men_cargo_pants/' manually.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Hyperstack Script\n",
                "!python3 hyperstack.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Visualize Output\n",
                "from IPython.display import Image, display\n",
                "import glob\n",
                "\n",
                "# Find overlay image\n",
                "overlays = glob.glob(\"output_data/**/*.jpg\", recursive=True)\n",
                "for overlay in overlays:\n",
                "    print(f\"Displaying {overlay}:\")\n",
                "    display(Image(filename=overlay, width=600))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}